################################ Getting Started #########################################

rm(list = ls())

packages <- list("ranger", "xgboost", "mda", "speedglm", "gam", 
                 "neuralnet", "class", "glmnet", "e1071", "caret", 
                 "parallel", "foreach", "doParallel", "tictoc", "beepr")

lapply(packages, library, character.only = TRUE) # load packages

path <- "~/Documents/Github/Empirical_Sudy_of_Ensemble_Learning_Methods/utils/"

files <- c("phon", "spam", "wdbc", "adult", "park", "higgs", "bcds", "vote", "clim",
           "begin_process", "end_process", "cv_parallel", "Kappa", "MCC",
           "binary", "ohe", "ohe.data.frame", "standardize", "tune_params")

for (i in 1:length(files)) { assign(files[i], readRDS(paste0(path, files[i]))) }

################################## Prepare Data ##########################################

df <- vote                                                         # current data set

holdout <- df[head(sample(1:nrow(df), nrow(df)), nrow(df)*0.2), ]  # 20% holdout sample
df <- df[!rownames(df) %in% rownames(holdout), ]                   # 80% training sample

tune_params(k = 2, c = 3, g = 15, df = df)                       # tune hyperparameters

# opts <- c("xgb.opt", # XGBoost
#           "xt.opt",  # extremely randomized trees
#           "rf.opt",  # random forest
#           "knn.opt", # k-nearest neighbors
#           "en.opt",  # elastic net params
#           "nn.opt",  # neural network
#           "svm.opt") # support vector machine

# retrieve hyperparameter placeholders to demo the ensemble-building process
# for (i in 1:length(opts)) { assign(opts[i], readRDS(paste0(path, opts[i]))) }

#########################################################################################
####################### APPLY MODELS TO THE TRAINING SAMPLE #############################
#########################################################################################

k <- 10        # number of folds in k-fold cross validation

CV.gam <- NULL   # generalized additive model with cubic smoothing splines
CV.knn <- NULL   # k-nearest neighbors
CV.rf  <- NULL   # random forest
CV.xt  <- NULL   # extremely randomized trees
CV.xgb <- NULL   # gradient boosted decision trees
CV.nn  <- NULL   # feedforward neural network trained by backprop and gradient descent
CV.glm <- NULL   # logistic regression
CV.en  <- NULL   # elastic net
CV.svm <- NULL   # support vector machine
CV.nb  <- NULL   # naive bayes
CV.fda <- NULL   # flexible discriminant analysis with multivar adaptive regression splines

L1_train <- NULL # level 1 data is generated by applying each of the base learners
                 # to the training sample and obtaining prediction probabilities
                 # during k-fold cross-validation

fold <- sample(cut(1:nrow(df), breaks = k, labels = FALSE))

for (i in 1:k) {
  
  # on rare occasions loop can fail when algorithms do not converge
  cat("Fold", i, "of", k, "\n")
  
  # partition into k folds
  train <- df[fold != i, ]
  test  <- df[fold == i, -1]
  
  # generalized additive model
  # apply smoothing splines to high-cardinality variables
  cardinality <- apply(within(train, rm("Y")), 2, function(x) length(unique(x)))
  formula <- as.formula(paste("Y ~", paste0(ifelse(cardinality > 9, "s(X", "X"), 1:ncol(train[ , -1]), 
                                            ifelse(cardinality > 9, ", df = 3)", ""), collapse = "+")))
  gam <- gam(formula, family = "binomial", data = train)
  CV.gam[i] <- Kappa(factor(round(predict(gam, test, type = "response"))), df$Y[fold == i])
  
  # random forest
  rf <- ranger(Y ~ . , mtry = rf.opt$mtry, max.depth = rf.opt$max.depth, 
               probability = TRUE, data = train, num.trees = 300)
  CV.rf[i] <- Kappa(factor(round(predict(rf, test)$predictions[ , 2])), df$Y[fold == i])
  
  # extremely randomized trees
  xt <- ranger(Y ~ . , mtry = xt.opt$mtry, max.depth = xt.opt$max.depth, 
               probability = TRUE, data = train, num.trees = 300, 
               splitrule = "extratrees", replace = F, sample.fraction = 1)
  CV.xt[i] <- Kappa(factor(round(predict(xt, test)$predictions[ , 2])), df$Y[fold == i])
  
  # XGBoost
  train_xgb <- sparse.model.matrix(Y ~ 0 + . , data = train)
  test_xgb <- sparse.model.matrix(Y ~ 0 + . , data = df[fold == i, ])
  
  params <- list(gamma = 0,
                 booster = "gbtree",
                 objective = "binary:logistic",
                 eta = xgb.opt["eta"],
                 subsample = xgb.opt["subsample"],
                 colsample_bytree = xgb.opt["colsample_bytree"],
                 colsample_bynode = xgb.opt["colsample_bynode"],
                 max_depth = xgb.opt["max_depth"])
  
  xgb <- xgboost(data = train_xgb, label = as.numeric(train$Y) - 1, 
                 params = params, nrounds = xgb.opt["trees"], verbose = FALSE)
  
  CV.xgb[i] <- Kappa(factor(round(predict(xgb, test_xgb))), df$Y[fold == i])
  
  # knn
  train_knn <- ohe(standardize(df))[fold != i, ]
  test_knn <- ohe(standardize(df))[fold == i, ]
  
  knn <- knn(train_knn, test_knn, cl = train$Y, k = knn.opt$neighbors, prob = TRUE)
  CV.knn[i] <- Kappa(knn, df$Y[fold == i])
  
  # logistic regression
  glm <- suppressWarnings(glm(Y ~ ., data = train, family = "binomial", control = list(maxit = 1000)))
  CV.glm[i] <- Kappa(factor(round(predict(glm, test, type = "response"))), df$Y[fold == i])
  
  # neural network
  train_nn <- ohe.data.frame(standardize(df))[fold != i, ]
  test_nn <- ohe.data.frame(standardize(df))[fold == i, -1]
  
  nn <- neuralnet(Y ~ . , data = train_nn, hidden = nn.opt$hidden, algorithm = nn.opt$algorithm,
                  rep = 1, stepmax = 1e4, linear.output = FALSE, threshold = 0.3)
  CV.nn[i] <- Kappa(factor(round(predict(nn, test_nn)[ , 2])), df$Y[fold == i])
  
  # elastic net
  train_en <- ohe(df)[fold != i, ]
  test_en <- ohe(df)[fold == i, ]
  
  en <- glmnet(train_en, train$Y, alpha = en.opt$alpha, lambda = en.opt$lambda,
               family = "binomial")
  CV.en[i] <- Kappa(factor(round(predict(en, test_en, type = "response"))), df$Y[fold == i])
  
  # support vector machine
  if (svm.opt$kernel == "linear") {
    svm <- svm(Y ~ . , data = train, kernel = svm.opt$kernel, cost = svm.opt$cost, 
               probability = TRUE, epsilon = svm.opt$epsilon)
  } else if (svm.opt$kernel == "radial") {
    svm <- svm(Y ~ . , data = train, kernel = svm.opt$kernel, cost = svm.opt$cost, 
               probability = TRUE, gamma = svm.opt$gamma)
  } else if (svm.opt$kernel == "polynomial") {
    svm <- svm(Y ~ . , data = train, kernel = svm.opt$kernel, cost = svm.opt$cost, 
               probability = TRUE, degree = svm.opt$degree, coef0 = svm.opt$coef0, epsilon = svm.opt$epsilon)
  }
  CV.svm[i] <- Kappa(predict(svm, test), df$Y[fold == i])
  
  # naive bayes
  nb <- naiveBayes(Y ~ . , data = train)
  CV.nb[i] <- Kappa(predict(nb, test), df$Y[fold == i])
  
  # flexible discriminant analysis
  fda <- fda(Y ~ ., data = train, method = mars)
  CV.fda[i] <- Kappa(predict(fda, test), df$Y[fold == i])
  
  # all predictions
  x <- data.frame("gam"    = predict(gam, test, type = "response"),
                  "knn"    = ifelse(knn == 1, attr(knn, "prob"), 1 - attr(knn, "prob")),
                  "rf"     = predict(rf, test)$predictions[ , 2],
                  "xt"     = predict(xt, test)$predictions[ , 2],
                  "xgb"    = predict(xgb, test_xgb),
                  "nn"     = predict(nn, test_nn)[ , 2],
                  "glm"    = predict(glm, test, type = "response"),
                  "en"     = predict(en, test_en, type = "response")[ , 1],
                  "svm"    = attr(predict(svm, test, probability = TRUE), "probabilities")[ , 1],
                  "nb"     = predict(nb, test, type = "raw")[ , 2],
                  "fda"    = predict(fda, test, type = "posterior")[ , 2],
                  "actual" = df$Y[fold == i])
  
  L1_train <- rbind.data.frame(L1_train, x) # append predictions to L1_train
  
}

train_loss <- apply(within(L1_train, rm("actual")), 2, function(x) {
  
  Kappa(factor(round(x), levels = c("0", "1")), L1_train$actual)
  
  })

heatmap(data.matrix(L1_train), Rowv = NA, xlab = "L1 Training Data")

#########################################################################################
################################ MAKE SOME PREDICTIONS ##################################
#########################################################################################

train <- df
test  <- holdout

# generalized additive model (cubic smoothing splines)
cardinality <- apply(within(train, rm("Y")), 2, function(x) length(unique(x)))
formula <- as.formula(paste("Y ~", paste0(ifelse(cardinality > 9, "s(X", "X"), 1:ncol(train[ , -1]), 
                                          ifelse(cardinality > 9, ", df = 3)", ""), collapse = "+")))
gam <- gam(formula, family = "binomial", data = train)

# random forest
rf <- ranger(Y ~ . , mtry = rf.opt$mtry, max.depth = rf.opt$max.depth, 
             probability = TRUE, data = train, num.trees = 300)

# extremely randomized trees
xt <- ranger(Y ~ . , mtry = xt.opt$mtry, max.depth = xt.opt$max.depth, 
             probability = TRUE, data = train, num.trees = 300, 
             splitrule = "extratrees", replace = F, sample.fraction = 1)

# XGBoost
train_xgb <- sparse.model.matrix(Y ~ 0 + . , data = train)
holdout_xgb <- sparse.model.matrix(Y ~ 0 + . , data = holdout)

params <- list(gamma = 0,
               booster = "gbtree",
               objective = "binary:logistic",
               eta = xgb.opt["eta"],
               subsample = xgb.opt["subsample"],
               colsample_bytree = xgb.opt["colsample_bytree"],
               colsample_bynode = xgb.opt["colsample_bynode"],
               max_depth = xgb.opt["max_depth"])

xgb <- xgboost(data = train_xgb, label = as.numeric(train$Y) - 1, 
               params = params, nrounds = xgb.opt["trees"], verbose = FALSE)

# knn
train_knn <- ohe(standardize(train))
holdout_knn <- ohe(standardize(holdout))

knn <- knn(train_knn, holdout_knn, cl = train$Y, k = knn.opt$neighbors, prob = TRUE)

# logistic regression
glm <- suppressWarnings(glm(Y ~ ., data = train, family = "binomial", control = list(maxit = 1000)))

# neural network
train_nn <- ohe.data.frame(standardize(train))
holdout_nn <- ohe.data.frame(standardize(holdout))[ , -1]

nn <- neuralnet(Y ~ . , data = train_nn, hidden = nn.opt$hidden, algorithm = nn.opt$algorithm,
                rep = 1, stepmax = 1e4, linear.output = FALSE, threshold = 0.3)

# elastic net
train_en <- ohe(train)
holdout_en <- ohe(holdout)

en <- glmnet(train_en, train$Y, alpha = en.opt$alpha, lambda = en.opt$lambda,
             family = "binomial")

# support vector machine
if (svm.opt$kernel == "linear") {
  svm <- svm(Y ~ . , data = train, kernel = svm.opt$kernel, cost = svm.opt$cost, 
             epsilon = svm.opt$epsilon, probability = TRUE)
} else if (svm.opt$kernel == "radial") {
  svm <- svm(Y ~ . , data = train, kernel = svm.opt$kernel, cost = svm.opt$cost, 
             gamma = svm.opt$gamma, probability = TRUE)
} else if (svm.opt$kernel == "polynomial") {
  svm <- svm(Y ~ . , data = train, kernel = svm.opt$kernel, cost = svm.opt$cost, 
             degree = svm.opt$degree, coef0 = svm.opt$coef0, epsilon = svm.opt$epsilon,
             probability = TRUE)
}

# naive bayes
nb <- naiveBayes(Y ~ . , data = train)

# flexible discriminant analysis (MARS)
fda <- fda(Y ~ ., data = train, method = mars)

# all predictions    
L1_holdout <- data.frame("gam"    = predict(gam, holdout, type = "response"),
                         "knn"    = ifelse(knn == 1, attr(knn, "prob"), 1 - attr(knn, "prob")),
                         "rf"     = predict(rf, holdout)$predictions[ , 2],
                         "xt"     = predict(xt, holdout)$predictions[ , 2],
                         "xgb"    = predict(xgb, holdout_xgb),
                         "nn"     = predict(nn, holdout_nn)[ , 2],
                         "glm"    = predict(glm, holdout, type = "response"),
                         "en"     = predict(en, holdout_en, type = "response")[ , 1],
                         "svm"    = attr(predict(svm, holdout, probability = TRUE), "probabilities")[ , 1],
                         "nb"     = predict(nb, holdout, type = "raw")[ , 2],
                         "fda"    = predict(fda, holdout, type = "posterior")[ , 2],
                         "actual" = holdout$Y)

valid_loss <- apply(within(L1_holdout, rm("actual")), 2, function(x) {
  
  Kappa(factor(round(x), levels = c("0", "1")), L1_holdout$actual)
  
})

heatmap(data.matrix(L1_holdout), Rowv = NA, xlab = "L1 Holdout Data")

#########################################################################################
########################### EVALUATE ALL POSSIBLE ENSEMBLES #############################
#########################################################################################

n <- length(train_loss)                                # number of base learners

mod_key <- colnames(within(L1_train, rm("actual")))    # names of the base learners

combns <- unlist(lapply(1:n, function(x) { 
  combn(1:n, x, simplify = FALSE)                      # all possible base learner combs
  }), recursive = FALSE)

best_in_training <- sapply(combns, function(x) max(train_loss[x]))

best_in_testing <- sapply(combns, function(x) max(valid_loss[x]))

# formula objects for all possible base learner combs
form <- lapply(combns, function(x) {
  paste("actual ~", paste(mod_key[x], collapse = " + "))
})

cat("Evaluating all possible OLS ensembles\n")
# ordinary least squares meta-learner
OLS_loss <- sapply(form, function(x) {
  mod <- speedglm(as.formula(x), family = binomial(), data = L1_train)
  probs <- predict(mod, within(L1_holdout, rm("actual")), type = "response")
  preds <- factor(round(probs), levels = c("0", "1"))
  loss <- Kappa(preds, L1_holdout$actual)
  return(loss)
})

cat("Evaluating all possible ridge regression ensembles\n")
# ridge regression meta-learner
ridge_loss <- sapply(combns, function(x) { 
  if (length(unlist(x)) < 2) {
    # not enough variables for ridge regression! default to basic glm
    mod <- glm(actual ~ . , data = L1_train[ , c(x, (n+1))], family = "binomial")
    # obtain the loss
    probs <- predict(mod, within(L1_holdout, rm("actual")), type = "response")
    preds <- factor(round(probs), levels = c("0", "1"))
    loss <- Kappa(preds, L1_holdout$actual)
  } else {
    # ridge regression
    mat <- model.matrix(~ . , L1_train[ , x])
    mod <- glmnet(mat, L1_train$actual, alpha = 0, family = "binomial", nlambda = 100)
    # obtain the loss
    mat <- model.matrix(~ . , L1_holdout[ , x])
    probs <- predict(mod, mat, type = "response", s = mod$lambda[which.max(mod$dev.ratio)])
    preds <- factor(round(probs), levels = c("0", "1"))
    loss <- Kappa(preds, L1_holdout$actual)
  }
  return(loss) # return loss
})

plot_diagnostics <- function(loss, metalearner) {
  
  plot(c(tail(loss, 1), rev(sort(valid_loss))), xlab = "Model", ylab = "Accuracy", 
       xaxt = "n", cex = .7, pch = 19, col = "firebrick",
       main = paste("Performance on Holdout Set with", metalearner, "Meta-Learner"))
  axis(1, at = 1:(1 + n), labels = c("meta", names(rev(sort(valid_loss)))))
  
  hist(loss/best_in_training - 1, breaks = 30, col = "firebrick",
       main = paste("% Improvement Over the Best Base Learner in the Training Sample\n with", 
                    metalearner, "Meta-Learner"),
       xlab = "% Improvement", ylab = "Number of Ensembles")
  
  percentimp <- round(mean(loss/best_in_training - 1 > 0)*100, 2) # percent improvement
  
  cat(paste0(percentimp, "% of ensembles do better\n"))
  cat(paste("The class ratio in the training sample was", 
            table(df$Y)[1]), "to", table(df$Y)[2])
  
  plot(rapply(combns, length), loss/best_in_testing - 1,
       xlab = "Numbers of Models", ylab = "% Improvement Over the Best Base Learner in the Holdout Sample",
       main = "Relationship Between Ensemble Performance and Number of Base Learners")
  abline(lm(loss/best_in_testing - 1 ~ rapply(combns, length)), col = "red")
  lines(lowess(loss/best_in_testing - 1 ~ rapply(combns, length)), col = "blue")
  
}

plot_diagnostics(OLS_loss, metalearner = "OLS")

plot_diagnostics(ridge_loss, metalearner = "Ridge")
